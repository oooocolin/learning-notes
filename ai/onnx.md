---
title: ONNX
tags:
  - AI
  - ONNX
---
## 一、概述
### 1. 背景
在一般的深度学习模型构建的流程中，常用的框架有 `PyTorch` 、 `TensorFlow` 等，一般都是在 `Python` 环境进行模型训练以及保存为特定框架的特定模型文件或是参数文件。这与框架强绑定，并且跨平台部署困难。所以 `ONNX` 应运而生，它可以实现各框架之间的相互转化，在这其中作为中间表示，并且对不同硬件平台统一接口，并做到了训练环境与部署环境分离。
### 2. 部署模型的选型
- 把模型保存为 `.onnx` 文件，使用 `ONNX Runtime` 在推理环境中加载。更适合跨平台场景。
- 转换为推理引擎的专用格式，可以更贴合特定的硬件，性能往往更高。
	- `TensorRT`：针对 `NVIDIA GPU` 优化。（需要转换成专门的 `engine` 文件）
	- `OpenVINO`：`Intel CPU/NPU` 优化。
	- `CoreML`：苹果设备（`iOS/macOS`）。
	- `TFLite`：移动端和边缘设备（主要是 `TensorFlow` 生态）。
- 封装为服务（常用于企业部署），以 `REST/gRPC` 服形式存在，由此可以进行负载均衡。
- 编译成可执行库或代码，可以使用在移动端直接嵌入 `APP`，如 `ncnn/MNN` 等。


